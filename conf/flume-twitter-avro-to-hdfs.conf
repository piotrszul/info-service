a1.channels = ch-1
a1.sources = src-1
a1.sinks = k1 

a1.sources.src-1.type = com.cloudera.flume.source.TwitterSource
a1.sources.src-1.channels = ch-1
a1.sources.src-1.consumerKey = YPntzEDhRj0UvU2iKkI6bm5dl
a1.sources.src-1.consumerSecret = ${CONSUMER_SECRET} 
a1.sources.src-1.accessToken = 764727998-7nnqWp8vff6hhQT7V6IvklWVrKLVT8rPojQGAvnZ
a1.sources.src-1.accessTokenSecret = ${ACCESS_TOKEN_SECRET}
a1.sources.src-1.keywords = tutorials point,java, bigdata, mapreduce, mahout, hbase, nosql
a1.sources.src-1.interceptors = json-to-avro
a1.sources.src-1.interceptors.json-to-avro.type = au.gov.dhs.flume.JsonToAvroConverter$Builder
a1.sources.src-1.interceptors.json-to-avro.schemaURL = hdfs://quickstart.cloudera:8020/user/cloudera/avro/tweet.avsc

a1.sinks.k1.type = logger
a1.sinks.k1.channel = ch-1

a1.sinks.HDFS.type = hdfs 
a1.sinks.HDFS.channel = ch-1
a1.sinks.HDFS.hdfs.path = hdfs://quickstart.cloudera:8020/user/cloudera/tweet_avro
a1.sinks.HDFS.hdfs.fileType = DataStream 
a1.sinks.HDFS.hdfs.writeFormat = Text 
a1.sinks.HDFS.hdfs.batchSize = 100
a1.sinks.HDFS.hdfs.rollSize = 0 
a1.sinks.HDFS.hdfs.rollInterval = 0
a1.sinks.HDFS.hdfs.rollCount = 1000
a1.sinks.HDFS.hdfs.serializer = avro_event
a1.sinks.HDFS.hdfs.serializer.schemaURL = file:///Users/szu004/contract/dhs/info-services/src/main/avro/tweet.avsc


a1.channels.ch-1.type = memory
a1.channels.ch-1.capacity = 10000
a1.channels.ch-1.transactionCapacity = 100

